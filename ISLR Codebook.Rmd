
```{r}
library(knitr)

pandoc('foo.md', format='html')  # HTML
pandoc('foo.md', format='latex') # LaTeX/PDF
pandoc('foo.md', format='docx')  # MS Word
pandoc('foo.md', format='odt')   # OpenDocument
```


---
output:
  html_document: default
  pdf_document: default
  word_document: default
---

Chap2 code review: 12/27
Chap3 code review: 12/28 - 1/5
Chap4 code review: 1/5 - 1/6
Chap5 code review: 1/6 -
Chap6
Chap7
Chap8
Chap9
Chap10


Introduction to Statistical Leaning by R
# Chapter2: Statistical learning
### 2.3.1 Basic command
```{r}
x=c(1,6,2)
y=c(1,4,3)
x
y
```

length:
calculate length of variables 
```{r}
length(x)
length(y)
```

ls(): 
list all of variables
rm(): 
delete any

```{r}
ls()
rm(x,y)
ls()
```

rm function(): 
remove all objects at once

```{r}
rm(list=ls()) #delete all objects
```

matrix function: 
creates a matrix of numbers

```{r}
?matrix
x=matrix(data=c(1,2,3,4),nrow=2,ncol=2)
x #omit data, nrow, ncol
```

sqrt function()
returns square root of each element of vector/matrix
```{r}
sqrt(x)
sqrt(x)^2
```

rnorm() function
generates a vector of random normal variables
```{r}
x=rnorm(50)
y=x+rnorm(50,mean=50,sd=0.1)
# default: mean=0, sd=1
```

set.seed() function
takes an arbitrary interger argument
set.seed is used to perform calculation involving random quantities
```{r}
set.seed(1303)
rnorm(50)

# delete all objects 
rm(list=ls()) 

set.seed(3)
y=rnorm(100)
mean(y)
var(y)
sqrt(var(y))
sd(y)

```

### 2.3.2 Graphics
The plot ()function is the primary way to plot data. 
```{r}
x=rnorm(100)
y=rnorm(100)
plot(x,y)
plot(x,y,xlab="x-axis", ylab="y-axis",)
```

When we want to save the output of an Rplot, pdf() function or jpeg() function can be used. 
- dev.off(): indicates to R that we are done creating the plot
```{r}
pdf("Figure.pdf")
plot(x,y,col="green")
dev.off()
```

the function seq can be used to create a sequence of numbers. For instance, seq(a,b) makes a vector of integers between a and b 
- seq(): 
```{r}
x=seq(1,10)
x
x=1:10
x
x=seq(-pi,pi,length=50)
x
seq(0,1,length=10)
```

We will now create more sophisticated plots, 
- contour(): produces contour plot to represent three-dimensional data similar to topographical map. It takes three arguments:
(1) A vector of the x values (the first dimension), 
(2) A vector of the y values (the second dimension), and
(3) A matrix whose elements correspond to the z value (the third dimension) for each pair of (x,y) coordinates. 

As with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour () function. 
```{r}
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45, add=T)
fa=(f-t(f))/2
contour(x,y,fa,nlevel=15)
```

The image() function works the same as contour (), except that it produces a color-coded plot whose colors depend on the z value. This is know as a heatmap.
- persp():produce a three-dimentional plot
          argument: theta and phi control angles

```{r}
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa,theta=30)
persp(x,y,fa,theta=30, phi=20)
persp(x,y,fa,theta=30, phi=70)
persp(x,y,fa,theta=30, phi=40)
```


### 2.3.3 Indexing Data
We often wish to examine part of a set of data.?@Suppose data is stored in the matrix A.Command [] will select element correspoding to specific row/column.We can also select multiple rows/columsn by providing vectors at the indices. 

R treats a single row/column of a matri as a vector.
The use of negative sign - in the index tell R to keep all rows/columns except these indicated in the index.
- dim(): outputs the number of rows followed by the number of columns of a given matrix

```{r}
A=matrix(1:16, 4,4)
A[2,3]
A[c(1,3), c(2,4)]
A[1:3, 2:4]
A[1:2,]
A[,1:2]

A[1,]
A[-c(1,3),]
dim(A)
```

### 2.3.4 Loading Data
data:Auto from ISLR library
- read.table(): primary function to load table data
- write.table():export data
- change dir...: select directory usingthe option under the File menu

Note that Auto.data is a text file, while you could alternatively open on PC using a standard text editor. Good idea to veiw a data set using a text editer or excel before loading it into R. 
```{r}
require(ISLR)
Auto=read.table("Auto.data")
fix(Auto)
```

This particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and has included them in the first row. 
The data set also includes a number of missing observations
- header=T (header=TRUE): first line of the file contains the variable names 
- na.strings:it sees a particular character or set of characters (such as question mark), which to be treated as a missing element of a data matrix. 
```{r}
Auto=read.table("Auto.data",header=TRUE, na.strings="?")
fixt(Auto)
```

#### Excel
Excel is a common-format data storage progmram.
- read.csv (): load csv data
```{r}
Auto=read.csv("Auto.csv",header=TRUE,na.strings="?")
fix(Auto)
dim(Auto)
Auto[1:4,]
```

The dim() function tells us that the data has 397 observation and nine variables(columns). There are various ways to deal with missing data.
- na.omit(): simply remove these rows
```{r}
Auto=na.omit(Auto)
dim(Auto)
```

Once the date are loaded correctly, we can use names () function to check the variable names.  
```{r}
names(Auto)
```

### 2.3.5 Additional graphical and numerical summaries 

We can use plot () function to produce scatterplots of the quantitative variables. 
```{r}
plot(Auto$cylinders,Auto$mpg)
attach(Auto)
plot(cylinders, mpg)
```

The cylinders variable is a numeric vector (quantitative), but it can be changed into factor variable, as there are only a small number of possible values. 
- as.factor(): converts quantiative variable into aqualitative variable
```{r}
cylinders=as.factor(cylinders)
```

If the variable plotted on the x-axix is categorical, then boxplots will automatically be procued with plot() function.
```{r}
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
plot(cylinders, mpg, col="red", varwidth=TRUE)
plot(cylinders, mpg, col="red", varwidth=TRUE,horizontal=TRUE)
plot(cylinders, mpg, col="red", varwidth=TRUE,xlab="cylinders",ylab="NPG")
```


-------------------------------------------------
# Chapter3: Linear Regression
### 3.6.1 Laboratories
```{r}
library(MASS)
install.packages("ISLR")
library(ISLR)
```

### 3.6.2 Simple Linear Regression
Using Boston dataset in the MASS package, we conduct a simple linear regression.variables in the Boston dataset is as follows.
- medv:median house value
- rm:average number of rooms
- age:average age of houses
- lstat:percent of households with low socioeconomic status

We simply apply lm () function for simple linear regression. Alternatively, Attach () function is used to define dataset.
```{r}
require(MASS)
fix(Boston)
names(Boston) #check variable list

lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
```

summary () function is useful to check regression results by linear model (lm).Addditionaly, the following functions are often used. 
- names():access other infoirmation stored in lm.fit
- coef(): extract estimated coefficients
```{r}
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
```

Confidential interval 
- confint(): show confidential internal
Predict 
- predict(): produce confidential interval/prediction interval (argument (5%, 10%, 15%) in the below illustration)
```{r}
predict(lm.fit, data.frame(lstat=(c(5,10,15))))
predict(lm.fit, data.frame(lstat=(c(5,10,15))),
        interval="prediction")
```

We now plot medv and lstat along with the least squares regression line using the plot() and abline () functions. 
- plot()
- abline(): used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type a bline(a,b). 
```{r}
plot(lstat, medv)
abline(lm.fit)
```

Below, we experiment with some additional settings for plotting lines and points. 
- lwd=#: width of the regression line to be increased by a factor of #
- pch: option to create different plotting symbols 
```{r}
plot(lstat,medv)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat, medv, col="red")
plot(lstat, medv, pch=20)
plot(lstat, medv, pch="+")
plot(lstat, medv, pch=1:20)
```

Next, we examine some diagnostic plots, several for which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot () function directly to output from lm().
- par(): split display screen into separate panels (2?~2 grid of panels)
```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

Alternatively, we can compute residuals from a linear regression fit using the residual () function. The function rstudent() will return the studentized residuals, an we can use this function to plot th resduals against the fitted values.
- residual():
- rstudent(): return studentied residuals
```{r}
plot(predict(lm.fit),residuals(lm.fit))
plot(predict(lm.fit),rstudent(lm.fit))
```

From the residual plot, there is some evidence of non-linearity. Leverage statistics can be computed for predictors using hatvalues() function.
- which.max()] identifies the index of the largest element of a vector
```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit)) #375
```

### 3.6.3 Multiple linear regression
In order to fit a multiple linear regression model using least squares, we use the lm() function. 

The syntax lm(y~x1+x2+x3) is used to fit a model with three predictors.
```{r}
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

The Boston data set contains 13 variables, and it would be cumbersum to have to type all of these in order to perform a regression using all of the predictors. Instead, we ca use the following shor-hand.
```{r}
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
```

We now access the individual components of a summary object by name. Hence, summary(lm.fit)$r.sq gives us the R2, and summary(lm.fit)$sigma gives us RSE. 
- The vif(): part of car package, compute variance inflation factors

```{r}
install.packages("car")
library(car)
vif(lm.fit)
```

When we want to exclude specific variables, the below syntax results in a regressiong using all predictors except age.
- update(): exclude specific variables 
```{r}
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit,~.-age)
```

### 3.6.4 Interaction Terms
It is easy to include interaction terms in a linear model using lm() function.

The sytntax lstat:black tells R to include an interaction term between lstat and black
```{r}
summary(lm(medv~lstat*age,data=Boston))
```

### 3.6.5 Non-linear Transformations of the Predictors

The lm() function can also accomodate non-linear transformations for the predictors. Given a predictor X, we can create a predictor X2 using I(X^2).
- I(): function needed to generate X to the power 2 (as ^ has a special meaning in a formula)
```{r}
install.packages("dply")
require(dplyr)
require(magrittr)

lm.fit2=lm(medv~lstat+I(lstat^2)) 
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. 
We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit. 
```{r}
lm.fit=lm(medv~lstat) 
anova(lm.fit, lm.fit2)
```

Here Model1 represents the linear sub-model containing only one predictor, lstat, while Model2 corresponds to the larer quadratic model that hs two predictors, lstat and lstat^2.
- anova(): performs a hypothesis test comparing the two models. 
  - Null: two models fit the data equally
  - Alternative:full model is superior 
  
Hence F-statistics is 135, and the associated p-value is virtually zero, which proides clear evidence that Model2 is far superior to Model1.
```{r}
par(mfrow=c(2,2))
plot(lm.fit2)

```

In order to create a cubic fit, we can include a predictor of the form I(x^3). However, this approach can start to get cumbersome for higher-order polynomials. Thus, poly() function to create the polynomial within lm(). 
- poly(): function to create polynomials.
```{r}
lm.fit5=lm(medv~poly(lstat,5)) 
summary(lm.fit5)
```

Including additional polynomial terms, up to fifth order leads to an improvement in the model fit. However, further investigation of the data revieals that no polynomials terms beyond fifth order have significant p-values in a regression fit. 
- log(): create log-based variable
```{r}
lm(medv~log(rm), data=Boston) %>% summary()
```


### 3.6.6 Qualitative Predictors

We will examine the Carseats data, which is part of the ISLR library. We will attempt to predict Sales in 400 locations based on a number of predictors. 
- dataset: Carseats (ISLR package)
```{r}
fix(Carseats) %>% names()
```

The Carseats dataset included following variables
- Sales: child car seat sales 
- Shelveloc: indicator of the quality of shelving location (space within a store in which the car seat is displayed, e.g., Bad, Medium, Good)

Give a qualitative variable, R generates dummy variables automatically. Below, we fit a multiple regression model that includes some interaction terms.
```{r}
lm(Sales~.+Income:Advertising:Age, data=Carseats) %>% summary()
```

R has created a ShelveLocGood  and ShelveLocMedium dummy variables. Both coefficients are positive, which means these variables contribute to sales increase(Magnitude is larger for ShelveLocGood variable).
- Contrast: returns the coding that R uses for the dummy variables. 
```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

### 3.6.7 Writing Functions
As we have seen, R comes with many useful functions. However, we will often intersted in perfoming an iperation for which no function is available. In this setting, we may wants to write our own function. For instance, below, we provide a simple function that reads in the ISLR and MASS libraries, called LoadLibraries().
- LoadLibraries(): ISLR and MASS

We create the function. Note that 
- +: the + symbols are printed by R and should not be typed in.
- {}: The symbols informs R that multiple commands are about to be input


```{r}
library(ISLR)

LoadLibraries=function()
{
  library(ISLR)
  library(MASS)
  print("The Libraries have been loaded.")
}

LoadLibraries
function()
  {
    library(ISLR)
    library(MASS)
    print("The libraries have been loaded.")
}

LoadLibraries()
```

-------------------------------------------------
# Chapter4: Classification
## 4.6: Lab - Logistic Regression, LDA, QDA and KNN

### 4.6.1 The Stock Market Data
We will begin by examining some numerical and graphical summaries of the Smarket data. 
- dataset: Smarket data (ISLR Library)
  - This dataset consists of percentage returuns for the S&P 500 stock index over 1,250 days (from 2001 - 2005)
  - Volume: the number of shares traded on the previous day
  - Today: the percentage return on the date in question
  - Direction:  whether the market was Up or Down on this date
  
```{r}
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
```

The cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a dataset. The first command below gives an error because the Direction variable is qualitative. 
```{r}
cor(Smarket)
cor(Smarket[,-9])
```

As one would expect, the correlation between the lag variables and today's returns are close to zero. There appears to be little correlation between today's returns and previous days' returns. The only substantial correlation is between Year and Volume. 

By plotting data we see thhat Volume is increasing over time. So to speak, the average number of shares traded daily incrased from 2001 t0 2005. 
```{r}
require(ggplot2)
names(Smarket)
attach(Smarket)
plot(Volume)
# ggplot(aes(x=Year, y=Volume),data=Smarket)+geom_point()
```

### 4.6.2 Logistic Regression
Next, we will fit a logistic regression in order to predict Direction using Lag1 through Lag5 and Volume.
- glm(): generalized linear models including logistic regression  
 - syntax (argument): family
 - logistic regression: family=binomial

```{r}
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket,
            family=binomial)
summary(glm.fit)
```

The smallest p-valueis associated with Lag1. The negative coefficients for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, the p-value is still large, and there is no clear evidence of real association between Lag1 and Direction. 
We use the coef() function to access the coefficients for this fitted model.

```{r}
coef(glm.fit)
summary(glm.fit)
summary(glm.fit)$coef
```

The predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type="response" option tells R to output probabilities of the form P(Y=1|X), as opposed to other information such as logit. 

If no data set is supplied to the predict() function, then the probabilieis are computed for the training data that was used to fit the logitic regression model. Here we have printed only the first ten probabilities. Noted that these values correspond to the probability of the market going up, rather than down (because contrast() function indicates that R has created a dummy variable with a 1 for Up).
```{r}
glm.probs=predict(glm.fit, type="response")
glm.probs[1:10]
contrasts(Direction)
```

In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following commands create a vector of class preditions based on whether the predicted probability of a market increase is greater  or less than 0.5.
```{r}
glm.pred=rep("Down",1250)
glm.pred[glm.probs>0.5]="Up"
```

- First: creates a vector of 1,250 Down elements
- Second:transforms to Up all elements for which the predicted probability of a market increase exceeds 0.5. 

Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. 
```{r}
table(glm.pred, Direction)
(507+145)/1250
mean(glm.pred==Direction)
```

The diagonal elements of the confusion matrix indicates correct predictions, while the off-diagonals represent incorrect predictions. Hence, our model corredctly predicted 52.2%...

However, this result is misleading because we trained nad tested the model on the same set of 1,250 obs.Thus, 100-52.2=47.8% is the training error rate. In order to assess the accuracy of the logistic regression model, we examine how well it precits the held-out data. 

To implement this strategy, we will first create a vector corresponding to obs from 2001 through 2004. Then, we will use this vector to create a held out dataset of observation from 2005. 

```{r}
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
```

The object train is a vector of 1,250 elements, corresponding to the observations in our dataset. The elements of the vector that correspond to observations in 2005 are set to TRUE, wherease those that correspond to observation in 2005 are set to FALSE. 
- Object train: Boolean vector

Boolean vectors can be used to obtain a subset of the rows/columns of matrix. the commance Smarket[train,] would pick out a submatrix of the stock market datadset, crresponding only to the dates before 2005.

We now fit a logistic regression model using only the subset of the observations that correspond to dates  before 2005, using the subset argument. We then obtrain predicted probabilities of the stock market going up for each of the day in our test set.

Notice that we have trained and tested our model on two completely separate data sets. Training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005, and compare them to the actual movements of the market over that time period.

```{r}

glm.fit2=glm(Direction~Lag1+Lag2,
            data=Smarket, family=binomial, subset=train)
glm.probs2=predict(glm.fit2,Smarket.2005,type="response")

glm.pred2=rep("Down",252)
glm.pred2[glm.probs2>0.5]="Up"

table(glm.pred,Direction.2005)
mean(glm.pred2==Direction.2005)
mean(glm.pred2!=Direction.2005)
106/(106+76)
```

Now the results appear to be more promissing: 56% of the daily movements have been correctly predicted. The confusion matrix suggests that on days when logistic regression predicts the market will decline, it is only correct 50% of the time. However, on days when it predicts an increase in the market, it has a 58% accuracy rate. 

Suppose we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction o a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and -0.8. We do this using the predict() function.

```{r}
predict(glm.fit2, newdata=data.frame(Lag1=c(1.2,1.5), Lag2=c(1.1,-0.8)),type="response")
```

### 4.6.3 Linear Discriminant Analysis
Now we will perform LDA on the Smarket data. In R, we fit a LDA model using the lda() function.
- lda(): MASS library
- lda(): syntax is identical to glm() except for the absense of the family option. 

We fit the model using only the observations before 2005.
```{r}
library(MASS)
library(ISLR)
lda.fit=lda(Direction~Lag1++Lag2, data=Smarket,subset=train)
lda.fit
plot(lda.fit)
```

The LDA output indicates that ??1^=0.492, ??2^=0.508 (49.2% of the training obs correspond to days during which the market went down). 
The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to from the LDA decision rule.

In other words, these are multipliers of the elements of X=x.If -0642 * Lag1 - 0.514*Lag2 is large, then the LDA classifier will predict a market increase, otherwise, LDA will predict a market decline.

The plot() function produces plots of the linear discriminants, obtained by computing -0.642*Lag1-0.514*+Lag2 for each of the training observations. 

The predict() function returns a list with three elements.
- first:class contains LDA's prediction about movement of the market.
- second:posterior is a matrix whose k-th column contains the posterior probability that the corresponding obs belongs to te kth class.
- third:x contains the linear descriminants

```{r}
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
# "class", "posterior", "x"
```

As we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.
```{r}
lda.class=lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
```

Applying a 50% threshold to the posterior probabilities allos us to recreate the predictions contained in lda.pred$class.
```{r}
sum(lda.pred$posterior[,1]>=.5)
sum(lda.pred$posterior[,1]<.5)
```

Notice that the posterior probability output by the model correspondends to the probability that the market will decrease:
```{r}
lda.pred$posterior[1:20,1]
lda.class[1:20]
```

If we wants to use a posterior distribution other than 50% to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are certain that the market will indeeed decrease on that day - say, if the posterior probability is least 90%.
```{r}
sum(lda.pred$posterior[,1]>.9)
```

No days in 2005 meet the threshold! In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%.


### 4.6.4 Quadratic Discriminant Analysis

We will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function.
- qda(): MASS library
 - syntax: identical to lda() function

```{r}
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
```

The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of predictors. 

The predict() function works in exactly the same fashon as for LDA.

```{r}
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
```

Interestingly, the QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model.

This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relation more accurately than linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method's performance on a larger test set before betting that this approach will beat the market.

### 4.6.5 K-Nearest Neighbors (K?ߖT?@)

We will now perform KNN using knn() function. This functions works differently from the other model-fitting functions that we have encounterd thus far.
- knn() function: class library

Rather than a two-step apprach in which we first fit the model and use the model to make predictions, knn() forms predictions using a single command. 

The function requires four inputs:

1. A matrix containing the predictors associated with the training data, labeled train.X below.
2. A matrix containing the predictors associated with the data for which we wish to make predictions, labeled test.X below. 3. A vector containing the class labels for the training observations, labeled train.Direction below.
4.  A value for K, the number of nearest neighborsto be used by the classifier.

We use the cbind() function, short for column bind, to bind the Lag1 and Lag2 variable together into two matrices, one for the training set and the other for the test set. 

```{r}
library(class)
train.X=cbind(Lag1,Lag2)[train,]
test.X=cbind(Lag1, Lag2)[!train,]
train.Direction=Direction[train]
```

Now the knn() function can be used to predict the market's movement for the data in 2005. We set a random seed before we apply knn() because if several obs are tied as nearest neighbors, then R will randomly break the tie. Thus,a seed must be set to ensure reproductivitiy of results.
```{r}
set.seed(1)
knn.pred=knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
(83+43)/252
```

The results using K=1 are not very good, since only 50% of the obs are correctly predicted. Of course, it maybe that K=1 results in an overly flexible fit to the data. Below, we repeat the analysis using K=3.
```{r}
knn.pred2=knn(train.X,test.X,train.Direction,k=3)
table(knn.pred2,Direction.2005)
mean(knn.pred2==Direction.2005)
```

The results have improved slightly. But increasing K further turns out to provide no further improvements. It appears that for this data, QDA provides the best results of the methods that we have examined so far.


### 4.6.6 An Application to Caravan Insurance Data

We will apply the KNN approach to the Caravan dataset.
- dataset: Caravan (ISLR library)
 - includes 85 predictors that measure demographic characteristics for 5822 individuals
 - Reponse variable is Purchase, which indicates whether or not a given individual purchase a caravan insurance policy
 - only 6% purchased caravan insurance
```{r}
library(ISLR)
dim(Caravan)
attach(Caravan)
summary(Purchase)
348/5822
```

Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. 

Any variables that are on a large scale will have a much larger effect on the distance between the obs , and on the KNN classifier, than variables that are on a small scale.

Example: 
- dataset that contains two variables, salary anage (measured in dollars/years)
- As far as KNN is concered, a difference of 1,000 in salary is enormous compared to a difference of 50 years in age
- Consequently, salary will drive the KNN classification results, and age will have almost no effect

A good way to handle this problem is to standardize the data so that all variables are give a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale.
- scale(): standardize

In addition, we exclude column 86, due to qualitative Purchase variables
```{r}
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
```

Now  every column of standardized.x has a standard deviation of one and a mean of zero.

We now splot the obs into a test set, containing first 1000 obs, and a training set, the remaining obs.

We fit a KNN model on the training data using K=1, and evaluate its performance on the test data.
```{r}
test=1:1000
train.X=standardized.x[-test,]
test.X=standardized.x[test,]

train.Y=Purchase[-test]
test.Y=Purchase[test]

set.seed(1)
knn.pred=knn(train.X, test.X,train.Y,k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
```

The vector test is numeric, with values from 1 through 1000. Typing standardized.X[test,] yields the submatrix of the data containing the obs whose indices range from 1 to 1000 whereas typing standardized.X[-test,] yields a submatrix containing the obs whose indices do not range from 1 to 1000.

The KNN error rate on the 1,000 test obs is just under 12%. At first glance, this may apprea to fairly good. However, since only 6% customer purchased insurance, we could get the error rate down to 6% of customers purchased insurance, we could get the error rate down to 6% by always predicting No regardless of the values of the predictors.

Suppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, a salesperson must visit each potential customer. If the company tries to sell insurance to a random selection of customers, then the success rate will be only 6%, which maybe far too low give the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to by it. So the overall error rate is not of interest. Instead, the fraction of individuals that are correctly predicted to buy insurance is of interst. 

It turns out that KNN with K=1 does far better than random guessing among customers that are predicted to buy insurance. Among 77 such customers, 9 or 11.7% actually do purchase insurance. This is double the rate that one would obtain from random guessing. 
```{r}
table(knn.pred,test.Y)
9/(68+9)
```

Using K=3, the success rate increases to 19%, and with K=5, the rate is 26.7%. This is over four times the reate that results from randome guessing. It appears that KNN is finding some real patterns in a difficult data set. 
```{r}
knn.pred2=knn(train.X,test.X,train.Y,k=3)
table(knn.pred2,test.Y)
5/26

knn.pred=knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
4/15
```

As a comparison, we can also fit a logistic regression model to the data. If we use 0.5 as the predicted probability cut-off for the clasifier, then we have a problem: only seven of the test obs are predicted to purchase insurance. 

However, we are required to us a cut-off of 0.5. If we instead predict a purchase any time the predicted probability of purchase exceeds 0.25, we get much better results: we predict that 33 will prchase insurance, and we are correct for about 33% of these people. This is over five times better than random guessing. 

```{r}
glm.fit=glm(Purchase~.,data=Caravan,family=binomial, subset=test)

glm.probs=predict(glm.fit,Caravan[test,],type="response")
glm.pred=rep("No",1000)
glm.pred[glm.probs<.5]="Yes"
table(glm.pred,test.Y)

glm.pred2=rep("No",1000)
glm.pred2[glm.probs<.25]="Yes"
table(glm.pred2,test.Y)
11/(22+11)
```



-------------------------------------------
# Chapter5: Resampling Methods 
## 5.3 Lab: Cross-Validation and the Bootstrap

We explore the resampling techniques covered in this chapter. Some of the commands in this lab may take while to run on PC.

### 5.3.1 THe validation set approach
We explore the use of the validation set approach to estimate the test error rates that result from fitting various linear models on the Auto dataset.
- Dataset: Auto (ISLR lbirary)

Before we begin, we use the set.seed() to set a seed for R's random number generator, so that the we can obtain precisely the same results. It is generally a good idea to set a random seed when performing an analysis such as cross-validation that contains an element or randomness.
- set.seed()

We begin by using the sample() to split the set of obs into two halves, by selecting a random subset of 196 obs out of the original 392 obs. We refer these obs as the training set. 
- sample()
```{r}
library(ISLR)
set.seed(1)
train=sample(392,196)

names(Auto)
?Auto
```

We then use the sbuset option to fit a linear regression using only the observations corresponding to the training set.
```{r}
lm.fit=lm(mpg~horsepower, data=Auto, subset=train)

require(ggplot2)
ggplot(aes(x=horsepower,y=mpg),data=Auto)+geom_line()
```

We now use the predict() function to estimate the response for all 392 observations, and we use the mean() function to calculate the MSE of the 196 observations in the validation set.

Note that the -train indext below selects only the observations that are not in the training set.
- mean()
```{r}
attach(Auto)
mean((mpg-predict(lm.fit,data=Auto))[-train]^2)
```

Therefore, the estimated test MSE for the linear regression fit is 26.14. We can use the poly() function to estimate the test error.
- poly(): estimate test error for polynomial/cubic regressions

```{r}
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,data=Auto))[-train]^2)

lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,data=Auto))[-train]^2)
```

These error rates are 19.82, and 19.78 respectively. If we choose a different training set instead, then we obtain different errors on the valudation set.

```{r}
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,data=Auto))[-train]^2)

lm.fit2=lm(mpg~poly(horsepower,2),subset=train)
mean((mpg-predict(lm.fit2,data=Auto))[-train]^2)

lm.fit2=lm(mpg~poly(horsepower,3),subset=train)
mean((mpg-predict(lm.fit3,data=Auto))[-train]^2)
```


### 5.3.2 Leave-One-Out Corss Validation (LOOCV)
The LOOCV estimate can be automatically computed for any generalized linerar model using the glm() and cv.glm() functions. 
IN the lab for Chapter4, we used the glm() function to perform logistic regression by passing in the family="binomial" argument. 

But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function. So for instance, 
- glm()
- cv.glm()
```{r}
glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)

lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
```

The above results yiedl identical liner regression models. In this lab, we will perform linear regression using the glm() function rather than the lm() function, because the latter can be used together with cv.glm(). 
cv.glm(): boot library
```{r}
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
```

The cv.glm() function produces a list with several components. The two numbers in the delta vector contain the corss-validation results.In this case, the numbers are identical (up to two decimal places), and correspond to LOOCV staistic given in (5.1). 

We discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately 24.23.

We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initate a for loop which iteratively fits polynomial regressions for polynomials of order i =1 to i = 5, computes the associated cross-validation error, and stores it in the ith element of the vector cv.error.
- for loop()
- cv.error()

We begin by initializing the vector. This command will likely take of a couple minutes to run.
```{r}
cv.error=rep(0,5)
for (i in 1:5) {
  glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
  cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]}
cv.error
plot(cv.error)

# cv.err=data.frame(cv.error)
# ggplot(aes(x=y=cv.error),data=cv.err)+geom_point()
```

We see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.


### 5.3.3. K-Fold Cross-Validation
The cv.glm() function can also be used to implement k-fold CV. Below we use k=10, a common choice for k, on the Auto data set. 

We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.

```{r}
set.seed(20)
cv.error.10=rep(0,10)
for(i in 1:10){
  glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
  cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[i]}
cv.error.10
```

Notice that the computation time is much shrter than that of LOOCV (In principle, the computation time for LOOCV for a least squares linear model should be faster than for K-fold CV, due to availability of the time formula (5,2) for LOOCV).

We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.

We saw in Sectioh 5.3.2 that the two numbers associated with delta are essentially the same when LOOCV is performed. When we instead perform k-fold CV, then the two numbers associated with delta differ slightly. The first is the standard k-fold CV estimate, as in (5.3). The second is a bias-corrected version. ON this data set, the two estimates are very similar to each other. 


### 5.3.4 The Bootsrap
We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as an example involving estimating the accuracy of the linear regression model on the Auto data set.

#### Estimating the Accuracy of a Statistic of Interest
One of the great advantage of the bootstrap approach is that it can be applied in almost all situations. No complicated mathmatical calculations are required.

Performing a bootstrap analysis in R entails only two steps.
1. We must create a function that computes the statistic of interest
2. We use the boot() function to perform the bootstrap by repeatedly sampling observations from the data set with replacement.
- boot(): boot library

The Portfolio dataset (ISLR package) in described in Section 5.2 To illustrate the use of the bootstrap on this data, we must create a function, alpha.fn(), which takes an input the (X,Y) data as well as a vector indicating which observations should be used to estimate α. The function then outputs the estimate for α based on the selected observations.
```{r}
alpha.fn=function(data,index){
  X=data$X[index]
  Y=data$Y[index]
  return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))}
```

This function retursns or outputs an estimate for α based on applying (5.7) to the observations indexed by the argument index. For instance, the following command tells R to estimate α using all 100 observations.
```{r}
alpha.fn(Portfolio,1:100)
```

The next command uses the sample() function to randomly select 100 observations from the range 1 t0 100, with replacement. This is equivalent to constructing a new bootstrap dataset and recomment α^ based on the new dataset.
- sample()
```{r}
set.seed(1)
alpha.fn(Portfolio, sample(100,100,replace=T))
```

We can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for α, and computing the resulting standard deviation. However, the boot() function automates this approach. Below, we produce R=1,000 bootstrap estimates for α.
- boot():
```{r}
boot(Portfolio, alpha.fn,R=1000)
```

The final output shows the using the original data, α^=0.5758, and that the bootstrap estimate for SE(α^) is 0.0886.

#### Estimating the accuracy of the linear regression model
The bootstrap approach can be used to asess the viariability of the coefficient estimates and predictions from a statistical learining method. 

Here we use the bootstrap approach in order to assess the variability of the estimates for β0 and β1, the intercept and slope terms for the linear regression model (uses horsepower to predict mpg in the Auto dataset). We will compare the estimates obtained using the bootstrap to those obtained using the formulas for SE(β0^) and SE(β1^) described in Section 3.1.2.

We first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. 
- boot.fn()

Then we apply this function to the full set of 392 observations in order to compute the estimates of Beta0, Beta1 on the entire data set using the usual linear regression coefficient estimate formulas from Chapter3. Note that we do not need the {and} at the beginning and end of the function because it is only one line long.

```{r}
boot.fn=function(data,index)+
return(coef(lm(mpg~horsepower, data=data, subset=index)))
boot.fn(Auto,1:392)
```

The boot.fin() function can be used in order to create bootstrap estimates for the interecept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.
- boot.fin()
```{r}
set.seed(1)
boot.fn(Auto, sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))
```

Next we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms. 
- boot()
```{r}
require(boot)
boot(Auto,boot.fn,1000)
```

This indicates that the bootstrap estimate for SE(Beta0) is 0.86, and that the bootstrap estimate for SE(Beta1) is 0.0074. As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regresion coefficients in a linear model. 

These can be obtained using the summary() function.
- summary()
```{r}
summary(lm(mpg~horsepower,data=Auto))$coef
```

The standard error estimates for $\hat{\beta_0}$ and $\hat{\beta_1}$ obtained using the formulas from Section 3.1.2 are 0.717 for te intercept and 0.0064 for the slope. 

Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggets the opposite. Recall that the standard formulas given in Equation 3.8 on page 66 rely on cetrain assumptions. 

For example, they depend on the unknown parameters $\sigma^2$, the noise variance. We then estimate $\sigma^2$ using the RSS. Now although the formula for the standard erros do not rely on the linear model being correct, the estimate for $\sigma^2$ does. 

We see in Figure 3.8 on page 91 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will $\hat{\sigma^2}$. 

Secondly, the standard formulas assume (somewhat unrealistically) that the $x_i$ are fixed, and all the variablity comes from the variation i the errors $\epsilon_i$. The bootstrap approach does not reply on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of $\hat{\beta_0}$ and $\hat{\beta_1}$ than is the summary function. 

Below we compute the bootstrap standard error estimates and the standard linear regression estimate that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of SE($\hat{\beta_0}$), SE($\hat{\beta_1}$) and SE($\hat{\beta_2}$).
```{r}
boot.fn=function(data,index)+
  coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,
                  subset=index))
set.seed(1)               
boot(Auto,boot.fn,1000)
```

-------------------------------------------
# Chapter 6: Linear Model Selection and Regularization

## 6.5 Lab1: Subset Selection Methods
### 6.5.1 Best Subset Selection

Here we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player's Salary on the basis of various statistics associated with performing in the previous year. 

First of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. 

It returs a vector of th same length at the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.
- is.na()
- sum()
```{r}

library(ISLR)
fix(Hitters)
names(Hitters)

dim(Hitters)
sum(is.na(Hitters$Salary))
```

Hence we see that Salary is missing for 59 players. The na.omit() function removes all of the rows that have missing values in any variable.
- na.omit():

```{r}
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

The regsubsets() function performs best subset selection by idenfifying the best model that contains a given number of predictors, where best is quantified using RSS. 
- regsubsets()

The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size. 
```{r}
install.packages("leaps")
library(leaps)

regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)
```

An asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. 

By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.

```{r}
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary=summary(regfit.full)
```

The summary() function also returns $R^2$, RSS, adjusted $R^2$, $C_p$ and BIC. We can examine these to try to select the best overall model.
```{r}
names(reg.summary)
```

For instance, we see that the $R^2$ statistic increased from 32%, when only one variable is included in the model, to almost 55%, when all variables are included. As expected the the $R^2$ statistic increases monotonically as more variables are included. 

```{r}
reg.summary$rsq
```

Plotting RSS, adjusted $R^2$ and BIC for all of the models at one will help us decide which model to select. Note the type="l" option tells R to connect the plotted points with lines. 
```{r}
par(mfrow=c(1,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
```

The points() command wokrs like te plot() command, except that it puts points on a plot that has already been created, instead of creating a new plot. 

The whch.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted $R^2$ statistic. 
- which.max()

```{r}
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
```

In a similar fashion, we can plot the $C_p$, and BIC statistics, and indicate the models with the smallest statistic using which.min().
- which.min()
```{r}
require(magrittr)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type="l") 
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)

which.min(reg.summary$cp) 
# [1] 10


which.min(reg.summary$bic)
# [1] 6

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
```

The regsubsets() function has a built-in plot() command which can be used to diplay the selected variables for the best model with a given number of predictors, ranked accordng to the BIC, $C_p$, adjusted $R^2$ or AIC. 
```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")

```

The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to -150. 

However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.

```{r}
coef(regfit.full,6)
```

### 6.5.2 Foward and Backward Stepwise Selection 





## 6.6 Lab2: Ridge Regression and the Lasso

we will use glmnet package for ridge regression and lasso.Unlike other functions, we must pass in an x matrix as well as y vector, not use y ~ x syntax for glmnet.

Dataset is Hitters data, and we predict Salary after removing missing values.
- Model.matrix(): create x by 1. producing correspondings to 19 predictors, (2) automatically transforms any qualitative variables into dummy variables. 
- glmnet(): the function only takes numerical values, thus quantitative inputs need to be transformed

```{r}
require(ISLR)
x=model.matrix(Salary~.,data=Hitters)[,-1]
y=Hitters$Salary
Hitters
```


### 6.6.1 Ridge Rgression

The glmnet()function has an alpha argument that determines what type of model to fit(alpha=0 -> ridge, alpha=1 -> lasso)

```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
```

In addition, the glmnet() function performs ridge regression for an automatically selected range of ?? values. In the above case, grid of values ?? is selected from ?? = 10^10 to 10^-2 (from only intercept to lewast squares fit).We can compute model fits for a particular value o ?? thats not one of the original grid values.

The glmnet() function standardizes the variables so that they are on the same scale. To cancel default setting, standardize=FALSE, cancel)

```{r}
#20?~100 matrix
dim(coef(ridge.mod)) 
```

We expect the coefficient estimates to be smaller, in terms of l2 norm, when a large value of ?? is used, as compared to when a small value of ?? is used. THese are coefficnents  when ??=11,498, along with their l2 norms. 
```{r}
## ?? = 11,498
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))

## ?? = 705
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

We can use the predict() function, to obtain ridge regression coefficients for a new value of ?? (??=50). 
```{r}
predict(ridge.mod,s=50,type="coefficients")[1:20,]
```


We now split samples into training set and test set to estimate test error. There are mainly two methods that we can apply.
- method (1): Produce a random vector of TRUE, FALSE elements and select obs corresponding to TRUE for the training data
- method (2): Randomly choose a sbuset of numbers between 1 and n as indices for the training set

We first set a random seed so that results obtained will be reproducible.
```{r}
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
tset=(-train)
y.test=y[test]
```

Next, we fit ridge regression on training set, evaluate its MSE (??=4). Note the use of predict() is to get predictions for test set, by replacing type="coefficients" with the newx argument. 
```{r}
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,
                 thres=1e-12)
ridge.pred=predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

The test MSE is 122072. Note that if we had simply fit model with just an intercept, we would have predicted each test obs using the mean of the training obs. In that case, we could compute the test set MSE as below.
```{r}
mean((mean(y[train])-y.test)^2)
```

We could also get the same results by fiting a ridge regression model with a very large value of ??. Note that, 1e10=10^10
```{r}
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)

```

Therefore, fitting a ridge regression model with ??=4 leads to much lower test MSE than fitting a model with just and intercept. We now check whether there is any benefit to performing ridge regression with ??=4 insted of just performing least square regressions. 

Recall that least squares is simply ridge regression with ??=0
```{r}
ridge.pred=predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x,subset=train)
predict(ridge.mod,s=0,type="coefficient")[1:20,]
```

In general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since the function provides more useful outputs such as se and p-values for coefficients.

We can do this using built-in cross validation functions, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument folds. Note that we set a random seed first so our results will be reproducible (the choice of cv folds is random)

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
```

From the above results, we see that the value of lambda that results in the smallest cross-validation error is 212. What is the test MSE associated with this value of lambda?

```{r}
ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

This represents a further improvement over the test MSE that we got using lambda=4. Finally, we refit our rife regression model on the full data set, using the value of lambda chosen by cv, and examine the coefficnent estimates. 

```{r}
out=glmnet(x,y,alpha=0)
predict(out, type="coefficients", s=bestlam)[1:20,]
```

As expected, none of the coefficents are zero - ridge regression does not perform variable selection!

###6.6.2 The Lasso
While the ridge regression with a wise choice of lambda can outperform least squares as well as the null model. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. 

In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. ther than this change, we proceed just as we did in fitting a ridge model.
```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```

We can see from the coefficient plot tat depending on the choice of tuning parameter (L1 norm), some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.
```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[train,])
mean((lasso.pred-y.test)^2)
```

This is subtantially lowe tha the test set MSE of the null model and of least squares and very similar to the test MSE of ridge ression with lambda chosen by cv.

However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 12 of the 19 coefficient estimates are exactly zero. So the lasso model with lambda chosen by cv contains only seven varables.

```{r}
out=glmnet(x,y,alpha=1, lambda=grid)
lasso.coef=predict(out, type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

## 6.7 Lab3 - PCR and PLS Regression
### 6.7.1 Principal Components Regression

Principalcomponents regression (PCR) can be perfromed using the pcr() function, which is a part of the pls library. We now apply PCR to the Hitters data, in order to predict Salary. Again, ensure that the missing values have been remived from the data. 

```{r}
install.packages("pls")
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~.,data=Hitters, sclae=TRUE,
            validation="CV")
```

The syntax for the pcr() function is similar to that for lm(),with a few additional options. 
- Setting scale=TRUE has effect of standardizing each predictor, using formula (6.6), prior to generating the principal components, so that the sacle on which each variable is measured will not have an effect. 
- Setting valudation="CV" causes pcr() to compute the ten-fold cross-validation error for each possible value of M, the number of principal components used. The resulting fit can be examined using summary().

```{r}
summary(pcr.fit)
```

The cv scores is provided for each possible number of components, ranging from M=0 onward (WE have printed the CV output only up to M=4). 
- pcr() reports the root mean squared error; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of 352.8 corresponds to an MSE of 352.8^2=124,468.

We can also plot the cross-validation scores using the validationplot() function.Using val.type="MSEP", will cause the cv MSE to be plotted.

```{r}
validationplot(pcr.fit,val.type="MSEP")
```

We see that the smallest cv erro occurs when M=16 components are ued. This is barely fewer than M = 19, which amounts to simly performing least squares, because when all of the components are used in PCR no dimension resduction occurs. However, from the plot we also see that the cv erro is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice. 

The summary () function also provides the percentage of variance explained in the predictors and in the responses using different numbers of components. This concept is discussed in greater detail in Chapter 10.

We can think of this as the amount of information about the predictors or setting M=1 only caputures 38.31% of all variance, or information in the predictors. In contract, M=6 increases the value to 88.63%. IF we were to use all M=p=19 components, this would increase to 100%. 

We now perform PCR on the training data and evaluate its test set performance. 

```{r}
set.seed(1)
pcr.fit=pcr(Salary~.,data=Hitters, subset=train, scale=TRUE,
            valudation="CV")
validationplot(pcr.fit, val.type="MSEP")
```

Now, we find that the lowest cv erro occurs when M=7 coponent are used. We compute the test MSE as follows. 
```{r}
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
```

This test set MSE is competitive with the results obtained using ridge ressionand the lasso. However, as a result of they way PCR is impletement, the final model is more difficult to interpret, because it does not perform any kind of variable selection or even direclty produce coefficent estimates.

Finally, we fit PCR on the full data set, using M=7, the number of components identified by cross-validation.

```{r}
pcr.fit=pcr(y~x, scale=TRUE,ncomp=7)
summary(pcr.fit)
```

### 6.7.2 Partial Least Squares 
We impletement partial least squares using the plsr() function, also in the pls library.
The syntax just like tha of the pcr() function.

```{r}
set.seed(1)
require(pls)
require(ISLR)
pls.fit=plsr(Salary~., data=Hitters, subset=train,scale=TRUE,
             validation="CV")
summary(pls.fit)
```

The lowest cross vxalidation error occurs when only M is two partial leastdirectionfs are used.?@

# Chapter 7: Moving beyond linearity
