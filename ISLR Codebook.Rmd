---
output:
  html_document: default
  pdf_document: default
  word_document: default
---

Chap2 code review: 12/27
Chap3 code review: 12/28 - 1/5
Chap4 code review: 1/5 - 1/6
Chap5 code review: 1/6 -
Chap6
Chap7
Chap8
Chap9
Chap10


Introduction to Statistical Leaning by R
# Chapter2: Statistical learning
### 2.3.1 Basic command
```{r}
x=c(1,6,2)
y=c(1,4,3)
x
y
```

length:
calculate length of variables 
```{r}
length(x)
length(y)
```

ls(): 
list all of variables
rm(): 
delete any

```{r}
ls()
rm(x,y)
ls()
```

rm function(): 
remove all objects at once

```{r}
rm(list=ls()) #delete all objects
```

matrix function: 
creates a matrix of numbers

```{r}
?matrix
x=matrix(data=c(1,2,3,4),nrow=2,ncol=2)
x #omit data, nrow, ncol
```

sqrt function()
returns square root of each element of vector/matrix
```{r}
sqrt(x)
sqrt(x)^2
```

rnorm() function
generates a vector of random normal variables
```{r}
x=rnorm(50)
y=x+rnorm(50,mean=50,sd=0.1)
# default: mean=0, sd=1
```

set.seed() function
takes an arbitrary interger argument
set.seed is used to perform calculation involving random quantities
```{r}
set.seed(1303)
rnorm(50)

# delete all objects 
rm(list=ls()) 

set.seed(3)
y=rnorm(100)
mean(y)
var(y)
sqrt(var(y))
sd(y)

```

### 2.3.2 Graphics
The plot ()function is the primary way to plot data. 
```{r}
x=rnorm(100)
y=rnorm(100)
plot(x,y)
plot(x,y,xlab="x-axis", ylab="y-axis",)
```

When we want to save the output of an Rplot, pdf() function or jpeg() function can be used. 
- dev.off(): indicates to R that we are done creating the plot
```{r}
pdf("Figure.pdf")
plot(x,y,col="green")
dev.off()
```

the function seq can be used to create a sequence of numbers. For instance, seq(a,b) makes a vector of integers between a and b 
- seq(): 
```{r}
x=seq(1,10)
x
x=1:10
x
x=seq(-pi,pi,length=50)
x
seq(0,1,length=10)
```

We will now create more sophisticated plots, 
- contour(): produces contour plot to represent three-dimensional data similar to topographical map. It takes three arguments:
(1) A vector of the x values (the first dimension), 
(2) A vector of the y values (the second dimension), and
(3) A matrix whose elements correspond to the z value (the third dimension) for each pair of (x,y) coordinates. 

As with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour () function. 
```{r}
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45, add=T)
fa=(f-t(f))/2
contour(x,y,fa,nlevel=15)
```

The image() function works the same as contour (), except that it produces a color-coded plot whose colors depend on the z value. This is know as a heatmap.
- persp():produce a three-dimentional plot
          argument: theta and phi control angles

```{r}
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa,theta=30)
persp(x,y,fa,theta=30, phi=20)
persp(x,y,fa,theta=30, phi=70)
persp(x,y,fa,theta=30, phi=40)
```


### 2.3.3 Indexing Data
We often wish to examine part of a set of data.Å@Suppose data is stored in the matrix A.Command [] will select element correspoding to specific row/column.We can also select multiple rows/columsn by providing vectors at the indices. 

R treats a single row/column of a matri as a vector.
The use of negative sign - in the index tell R to keep all rows/columns except these indicated in the index.
- dim(): outputs the number of rows followed by the number of columns of a given matrix

```{r}
A=matrix(1:16, 4,4)
A[2,3]
A[c(1,3), c(2,4)]
A[1:3, 2:4]
A[1:2,]
A[,1:2]

A[1,]
A[-c(1,3),]
dim(A)
```

### 2.3.4 Loading Data
data:Auto from ISLR library
- read.table(): primary function to load table data
- write.table():export data
- change dir...: select directory usingthe option under the File menu

Note that Auto.data is a text file, while you could alternatively open on PC using a standard text editor. Good idea to veiw a data set using a text editer or excel before loading it into R. 
```{r}
require(ISLR)
Auto=read.table("Auto.data")
fix(Auto)
```

This particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and has included them in the first row. 
The data set also includes a number of missing observations
- header=T (header=TRUE): first line of the file contains the variable names 
- na.strings:it sees a particular character or set of characters (such as question mark), which to be treated as a missing element of a data matrix. 
```{r}
Auto=read.table("Auto.data",header=TRUE, na.strings="?")
fixt(Auto)
```

#### Excel
Excel is a common-format data storage progmram.
- read.csv (): load csv data
```{r}
Auto=read.csv("Auto.csv",header=TRUE,na.strings="?")
fix(Auto)
dim(Auto)
Auto[1:4,]
```

The dim() function tells us that the data has 397 observation and nine variables(columns). There are various ways to deal with missing data.
- na.omit(): simply remove these rows
```{r}
Auto=na.omit(Auto)
dim(Auto)
```

Once the date are loaded correctly, we can use names () function to check the variable names.  
```{r}
names(Auto)
```

### 2.3.5 Additional graphical and numerical summaries 

We can use plot () function to produce scatterplots of the quantitative variables. 
```{r}
plot(Auto$cylinders,Auto$mpg)
attach(Auto)
plot(cylinders, mpg)
```

The cylinders variable is a numeric vector (quantitative), but it can be changed into factor variable, as there are only a small number of possible values. 
- as.factor(): converts quantiative variable into aqualitative variable
```{r}
cylinders=as.factor(cylinders)
```

If the variable plotted on the x-axix is categorical, then boxplots will automatically be procued with plot() function.
```{r}
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
plot(cylinders, mpg, col="red", varwidth=TRUE)
plot(cylinders, mpg, col="red", varwidth=TRUE,horizontal=TRUE)
plot(cylinders, mpg, col="red", varwidth=TRUE,xlab="cylinders",ylab="NPG")
```


-------------------------------------------------
# Chapter3: Linear Regression
### 3.6.1 Laboratories
```{r}
library(MASS)
install.packages("ISLR")
library(ISLR)
```

### 3.6.2 Simple Linear Regression
Using Boston dataset in the MASS package, we conduct a simple linear regression.variables in the Boston dataset is as follows.
- medv:median house value
- rm:average number of rooms
- age:average age of houses
- lstat:percent of households with low socioeconomic status

We simply apply lm () function for simple linear regression. Alternatively, Attach () function is used to define dataset.
```{r}
require(MASS)
fix(Boston)
names(Boston) #check variable list

lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
```

summary () function is useful to check regression results by linear model (lm).Addditionaly, the following functions are often used. 
- names():access other infoirmation stored in lm.fit
- coef(): extract estimated coefficients
```{r}
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
```

Confidential interval 
- confint(): show confidential internal
Predict 
- predict(): produce confidential interval/prediction interval (argument (5%, 10%, 15%) in the below illustration)
```{r}
predict(lm.fit, data.frame(lstat=(c(5,10,15))))
predict(lm.fit, data.frame(lstat=(c(5,10,15))),
        interval="prediction")
```

We now plot medv and lstat along with the least squares regression line using the plot() and abline () functions. 
- plot()
- abline(): used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type a bline(a,b). 
```{r}
plot(lstat, medv)
abline(lm.fit)
```

Below, we experiment with some additional settings for plotting lines and points. 
- lwd=#: width of the regression line to be increased by a factor of #
- pch: option to create different plotting symbols 
```{r}
plot(lstat,medv)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat, medv, col="red")
plot(lstat, medv, pch=20)
plot(lstat, medv, pch="+")
plot(lstat, medv, pch=1:20)
```

Next, we examine some diagnostic plots, several for which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot () function directly to output from lm().
- par(): split display screen into separate panels (2Å~2 grid of panels)
```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

Alternatively, we can compute residuals from a linear regression fit using the residual () function. The function rstudent() will return the studentized residuals, an we can use this function to plot th resduals against the fitted values.
- residual():
- rstudent(): return studentied residuals
```{r}
plot(predict(lm.fit),residuals(lm.fit))
plot(predict(lm.fit),rstudent(lm.fit))
```

From the residual plot, there is some evidence of non-linearity. Leverage statistics can be computed for predictors using hatvalues() function.
- which.max()] identifies the index of the largest element of a vector
```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit)) #375
```

### 3.6.3 Multiple linear regression
In order to fit a multiple linear regression model using least squares, we use the lm() function. 

The syntax lm(y~x1+x2+x3) is used to fit a model with three predictors.
```{r}
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

The Boston data set contains 13 variables, and it would be cumbersum to have to type all of these in order to perform a regression using all of the predictors. Instead, we ca use the following shor-hand.
```{r}
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
```

We now access the individual components of a summary object by name. Hence, summary(lm.fit)$r.sq gives us the R2, and summary(lm.fit)$sigma gives us RSE. 
- The vif(): part of car package, compute variance inflation factors

```{r}
install.packages("car")
library(car)
vif(lm.fit)
```

When we want to exclude specific variables, the below syntax results in a regressiong using all predictors except age.
- update(): exclude specific variables 
```{r}
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit,~.-age)
```

### 3.6.4 Interaction Terms
It is easy to include interaction terms in a linear model using lm() function.

The sytntax lstat:black tells R to include an interaction term between lstat and black
```{r}
summary(lm(medv~lstat*age,data=Boston))
```

### 3.6.5 Non-linear Transformations of the Predictors

The lm() function can also accomodate non-linear transformations for the predictors. Given a predictor X, we can create a predictor X2 using I(X^2).
- I(): function needed to generate X to the power 2 (as ^ has a special meaning in a formula)
```{r}
install.packages("dply")
require(dplyr)
require(magrittr)

lm.fit2=lm(medv~lstat+I(lstat^2)) 
summary(lm.fit2)
```

The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. 
We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit. 
```{r}
lm.fit=lm(medv~lstat) 
anova(lm.fit, lm.fit2)
```

Here Model1 represents the linear sub-model containing only one predictor, lstat, while Model2 corresponds to the larer quadratic model that hs two predictors, lstat and lstat^2.
- anova(): performs a hypothesis test comparing the two models. 
  - Null: two models fit the data equally
  - Alternative:full model is superior 
  
Hence F-statistics is 135, and the associated p-value is virtually zero, which proides clear evidence that Model2 is far superior to Model1.
```{r}
par(mfrow=c(2,2))
plot(lm.fit2)

```

In order to create a cubic fit, we can include a predictor of the form I(x^3). However, this approach can start to get cumbersome for higher-order polynomials. Thus, poly() function to create the polynomial within lm(). 
- poly(): function to create polynomials.
```{r}
lm.fit5=lm(medv~poly(lstat,5)) 
summary(lm.fit5)
```

Including additional polynomial terms, up to fifth order leads to an improvement in the model fit. However, further investigation of the data revieals that no polynomials terms beyond fifth order have significant p-values in a regression fit. 
- log(): create log-based variable
```{r}
lm(medv~log(rm), data=Boston) %>% summary()
```


### 3.6.6 Qualitative Predictors

We will examine the Carseats data, which is part of the ISLR library. We will attempt to predict Sales in 400 locations based on a number of predictors. 
- dataset: Carseats (ISLR package)
```{r}
fix(Carseats) %>% names()
```

The Carseats dataset included following variables
- Sales: child car seat sales 
- Shelveloc: indicator of the quality of shelving location (space within a store in which the car seat is displayed, e.g., Bad, Medium, Good)

Give a qualitative variable, R generates dummy variables automatically. Below, we fit a multiple regression model that includes some interaction terms.
```{r}
lm(Sales~.+Income:Advertising:Age, data=Carseats) %>% summary()
```

R has created a ShelveLocGood  and ShelveLocMedium dummy variables. Both coefficients are positive, which means these variables contribute to sales increase(Magnitude is larger for ShelveLocGood variable).
- Contrast: returns the coding that R uses for the dummy variables. 
```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

### 3.6.7 Writing Functions
As we have seen, R comes with many useful functions. However, we will often intersted in perfoming an iperation for which no function is available. In this setting, we may wants to write our own function. For instance, below, we provide a simple function that reads in the ISLR and MASS libraries, called LoadLibraries().
- LoadLibraries(): ISLR and MASS

We create the function. Note that 
- +: the + symbols are printed by R and should not be typed in.
- {}: The symbols informs R that multiple commands are about to be input


```{r}
library(ISLR)

LoadLibraries=function()
{
  library(ISLR)
  library(MASS)
  print("The Libraries have been loaded.")
}

LoadLibraries
function()
  {
    library(ISLR)
    library(MASS)
    print("The libraries have been loaded.")
}

LoadLibraries()
```

-------------------------------------------------
# Chapter4: Classification
## 4.6: Lab - Logistic Regression, LDA, QDA and KNN

### 4.6.1 The Stock Market Data
We will begin by examining some numerical and graphical summaries of the Smarket data. 
- dataset: Smarket data (ISLR Library)
  - This dataset consists of percentage returuns for the S&P 500 stock index over 1,250 days (from 2001 - 2005)
  - Volume: the number of shares traded on the previous day
  - Today: the percentage return on the date in question
  - Direction:  whether the market was Up or Down on this date
  
```{r}
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
```

The cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a dataset. The first command below gives an error because the Direction variable is qualitative. 
```{r}
cor(Smarket)
cor(Smarket[,-9])
```

As one would expect, the correlation between the lag variables and today's returns are close to zero. There appears to be little correlation between today's returns and previous days' returns. The only substantial correlation is between Year and Volume. 

By plotting data we see thhat Volume is increasing over time. So to speak, the average number of shares traded daily incrased from 2001 t0 2005. 
```{r}
require(ggplot2)
names(Smarket)
attach(Smarket)
plot(Volume)
# ggplot(aes(x=Year, y=Volume),data=Smarket)+geom_point()
```

### 4.6.2 Logistic Regression
Next, we will fit a logistic regression in order to predict Direction using Lag1 through Lag5 and Volume.
- glm(): generalized linear models including logistic regression  
 - syntax (argument): family
 - logistic regression: family=binomial

```{r}
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket,
            family=binomial)
summary(glm.fit)
```

The smallest p-valueis associated with Lag1. The negative coefficients for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, the p-value is still large, and there is no clear evidence of real association between Lag1 and Direction. 
We use the coef() function to access the coefficients for this fitted model.

```{r}
coef(glm.fit)
summary(glm.fit)
summary(glm.fit)$coef
```

The predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type="response" option tells R to output probabilities of the form P(Y=1|X), as opposed to other information such as logit. 

If no data set is supplied to the predict() function, then the probabilieis are computed for the training data that was used to fit the logitic regression model. Here we have printed only the first ten probabilities. Noted that these values correspond to the probability of the market going up, rather than down (because contrast() function indicates that R has created a dummy variable with a 1 for Up).
```{r}
glm.probs=predict(glm.fit, type="response")
glm.probs[1:10]
contrasts(Direction)
```

In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following commands create a vector of class preditions based on whether the predicted probability of a market increase is greater  or less than 0.5.
```{r}
glm.pred=rep("Down",1250)
glm.pred[glm.probs>0.5]="Up"
```

- First: creates a vector of 1,250 Down elements
- Second:transforms to Up all elements for which the predicted probability of a market increase exceeds 0.5. 

Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified. 
```{r}
table(glm.pred, Direction)
(507+145)/1250
mean(glm.pred==Direction)
```

The diagonal elements of the confusion matrix indicates correct predictions, while the off-diagonals represent incorrect predictions. Hence, our model corredctly predicted 52.2%...

However, this result is misleading because we trained nad tested the model on the same set of 1,250 obs.Thus, 100-52.2=47.8% is the training error rate. In order to assess the accuracy of the logistic regression model, we examine how well it precits the held-out data. 

To implement this strategy, we will first create a vector corresponding to obs from 2001 through 2004. Then, we will use this vector to create a held out dataset of observation from 2005. 

```{r}
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
```

The object train is a vector of 1,250 elements, corresponding to the observations in our dataset. The elements of the vector that correspond to observations in 2005 are set to TRUE, wherease those that correspond to observation in 2005 are set to FALSE. 
- Object train: Boolean vector

Boolean vectors can be used to obtain a subset of the rows/columns of matrix. the commance Smarket[train,] would pick out a submatrix of the stock market datadset, crresponding only to the dates before 2005.

We now fit a logistic regression model using only the subset of the observations that correspond to dates  before 2005, using the subset argument. We then obtrain predicted probabilities of the stock market going up for each of the day in our test set.

Notice that we have trained and tested our model on two completely separate data sets. Training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005, and compare them to the actual movements of the market over that time period.

```{r}

glm.fit2=glm(Direction~Lag1+Lag2,
            data=Smarket, family=binomial, subset=train)
glm.probs2=predict(glm.fit2,Smarket.2005,type="response")

glm.pred2=rep("Down",252)
glm.pred2[glm.probs2>0.5]="Up"

table(glm.pred,Direction.2005)
mean(glm.pred2==Direction.2005)
mean(glm.pred2!=Direction.2005)
106/(106+76)
```

Now the results appear to be more promissing: 56% of the daily movements have been correctly predicted. The confusion matrix suggests that on days when logistic regression predicts the market will decline, it is only correct 50% of the time. However, on days when it predicts an increase in the market, it has a 58% accuracy rate. 

Suppose we want to predict the returns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction o a day when Lag1 and Lag2 equal 1.2 and 1.1, respectively, and on a day when they equal 1.5 and -0.8. We do this using the predict() function.

```{r}
predict(glm.fit2, newdata=data.frame(Lag1=c(1.2,1.5), Lag2=c(1.1,-0.8)),type="response")
```

### 4.6.3 Linear Discriminant Analysis
Now we will perform LDA on the Smarket data. In R, we fit a LDA model using the lda() function.
- lda(): MASS library
- lda(): syntax is identical to glm() except for the absense of the family option. 

We fit the model using only the observations before 2005.
```{r}
library(MASS)
library(ISLR)
lda.fit=lda(Direction~Lag1++Lag2, data=Smarket,subset=train)
lda.fit
plot(lda.fit)
```

The LDA output indicates that ÉŒ1^=0.492, ÉŒ2^=0.508 (49.2% of the training obs correspond to days during which the market went down). 

The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to from the LDA decision rule.

In other words, these are multipliers of the elements of X=x.If -0642 * Lag1 - 0.514*Lag2 is large, then the LDA classifier will predict a market increase, otherwise, LDA will predict a market decline.

The plot() function produces plots of the linear discriminants, obtained by computing -0.642*Lag1-0.514*+Lag2 for each of the training observations. 

The predict() function returns a list with three elements.
- first:class contains LDA's prediction about movement of the market.
- second:posterior is a matrix whose k-th column contains the posterior probability that the corresponding obs belongs to te kth class.
- third:x contains the linear descriminants

```{r}
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
# "class", "posterior", "x"
```

As we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.
```{r}
lda.class=lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
```

Applying a 50% threshold to the posterior probabilities allos us to recreate the predictions contained in lda.pred$class.
```{r}
sum(lda.pred$posterior[,1]>=.5)
sum(lda.pred$posterior[,1]<.5)
```

Notice that the posterior probability output by the model correspondends to the probability that the market will decrease:
```{r}
lda.pred$posterior[1:20,1]
lda.class[1:20]
```

If we wants to use a posterior distribution other than 50% to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are certain that the market will indeeed decrease on that day - say, if the posterior probability is least 90%.
```{r}
sum(lda.pred$posterior[,1]>.9)
```

No days in 2005 meet the threshold! In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%.


### 4.6.4 Quadratic Discriminant Analysis

We will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function.
- qda(): MASS library
 - syntax: identical to lda() function

```{r}
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
```

The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of predictors. 

The predict() function works in exactly the same fashon as for LDA.

```{r}
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
```

Interestingly, the QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model.

This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relation more accurately than linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method's performance on a larger test set before betting that this approach will beat the market.

### 4.6.5 K-Nearest Neighbors (KãﬂñTñ@)

We will now perform KNN using knn() function. This functions works differently from the other model-fitting functions that we have encounterd thus far.
- knn() function: class library

Rather than a two-step apprach in which we first fit the model and use the model to make predictions, knn() forms predictions using a single command. 

The function requires four inputs
1 A matrix containing the predictors associated with the training data, labeled train.X below.
2 
3 
4 


```{r}

```


### 4.6.6 An Application to Caravan Insurance Data

```{r}

```


# Chapter5: Resampling Methods 












-------------------------------------------------
##6.6 Lab2 Ridge Regression and the Lasso

we will use glmnet package for ridge regression and lasso.Unlike other functions, we must pass in an x matrix as well as y vector, not use y ~ x syntax for glmnet.

Dataset is Hitters data, and we predict Salary after removing missing values.
- Model.matrix(): create x by 1. producing correspondings to 19 predictors, (2) automatically transforms any qualitative variables into dummy variables. 
- glmnet(): the function only takes numerical values, thus quantitative inputs need to be transformed

```{r}
require(ISLR)
x=model.matrix(Salary~.,data=Hitters)[,-1]
y=Hitters$Salary
Hitters
```


### 6.6.1 Ridge Rgression

The glmnet()function has an alpha argument that determines what type of model to fit(alpha=0 -> ridge, alpha=1 -> lasso)

```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
```

In addition, the glmnet() function performs ridge regression for an automatically selected range of É… values. In the above case, grid of values É… is selected from É… = 10^10 to 10^-2 (from only intercept to lewast squares fit).We can compute model fits for a particular value o É… thats not one of the original grid values.

The glmnet() function standardizes the variables so that they are on the same scale. To cancel default setting, standardize=FALSE, cancel)

```{r}
#20Å~100 matrix
dim(coef(ridge.mod)) 
```

We expect the coefficient estimates to be smaller, in terms of l2 norm, when a large value of É… is used, as compared to when a small value of É… is used. THese are coefficnents  when É…=11,498, along with their l2 norms. 
```{r}
## É… = 11,498
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))

## É… = 705
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

We can use the predict() function, to obtain ridge regression coefficients for a new value of É… (É…=50). 
```{r}
predict(ridge.mod,s=50,type="coefficients")[1:20,]
```


We now split samples into training set and test set to estimate test error. There are mainly two methods that we can apply.
- method (1): Produce a random vector of TRUE, FALSE elements and select obs corresponding to TRUE for the training data
- method (2): Randomly choose a sbuset of numbers between 1 and n as indices for the training set

We first set a random seed so that results obtained will be reproducible.
```{r}
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
tset=(-train)
y.test=y[test]
```

Next, we fit ridge regression on training set, evaluate its MSE (É…=4). Note the use of predict() is to get predictions for test set, by replacing type="coefficients" with the newx argument. 
```{r}
ridge.mod=glmnet(x[train,],y[train],alpha=0, lambda=grid,
                 thres=1e-12)
ridge.pred=predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

The test MSE is 122072. Note that if we had simply fit model with just an intercept, we would have predicted each test obs using the mean of the training obs. In that case, we could compute the test set MSE as below.
```{r}
mean((mean(y[train])-y.test)^2)
```

We could also get the same results by fiting a ridge regression model with a very large value of É…. Note that, 1e10=10^10
```{r}
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)

```

Therefore, fitting a ridge regression model with É…=4 leads to much lower test MSE than fitting a model with just and intercept. We now check whether there is any benefit to performing ridge regression with É…=4 insted of just performing least square regressions. 

Recall that least squares is simply ridge regression with É…=0
```{r}
ridge.pred=predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred-y.test)^2)
lm(y~x,subset=train)
predict(ridge.mod,s=0,type="coefficient")[1:20,]
```

In general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since the function provides more useful outputs such as se and p-values for coefficients.

We can do this using built-in cross validation functions, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument folds. Note that we set a random seed first so our results will be reproducible (the choice of cv folds is random)

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
```

From the above results, we see that the value of lambda that results in the smallest cross-validation error is 212. What is the test MSE associated with this value of lambda?

```{r}
ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

This represents a further improvement over the test MSE that we got using lambda=4. Finally, we refit our rife regression model on the full data set, using the value of lambda chosen by cv, and examine the coefficnent estimates. 

```{r}
out=glmnet(x,y,alpha=0)
predict(out, type="coefficients", s=bestlam)[1:20,]
```

As expected, none of the coefficents are zero - ridge regression does not perform variable selection!

###6.6.2 The Lasso
While the ridge regression with a wise choice of lambda can outperform least squares as well as the null model. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. 

In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. ther than this change, we proceed just as we did in fitting a ridge model.
```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```

We can see from the coefficient plot tat depending on the choice of tuning parameter (L1 norm), some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.
```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[train,])
mean((lasso.pred-y.test)^2)
```

This is subtantially lowe tha the test set MSE of the null model and of least squares and very similar to the test MSE of ridge ression with lambda chosen by cv.

However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 12 of the 19 coefficient estimates are exactly zero. So the lasso model with lambda chosen by cv contains only seven varables.

```{r}
out=glmnet(x,y,alpha=1, lambda=grid)
lasso.coef=predict(out, type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

## 6.7 Lab3 - PCR and PLS Regression
### 6.7.1 Principal Components Regression

Principalcomponents regression (PCR) can be perfromed using the pcr() function, which is a part of the pls library. We now apply PCR to the Hitters data, in order to predict Salary. Again, ensure that the missing values have been remived from the data. 

```{r}
install.packages("pls")
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~.,data=Hitters, sclae=TRUE,
            validation="CV")
```

The syntax for the pcr() function is similar to that for lm(),with a few additional options. 
- Setting scale=TRUE has effect of standardizing each predictor, using formula (6.6), prior to generating the principal components, so that the sacle on which each variable is measured will not have an effect. 
- Setting valudation="CV" causes pcr() to compute the ten-fold cross-validation error for each possible value of M, the number of principal components used. The resulting fit can be examined using summary().

```{r}
summary(pcr.fit)
```

The cv scores is provided for each possible number of components, ranging from M=0 onward (WE have printed the CV output only up to M=4). 
- pcr() reports the root mean squared error; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of 352.8 corresponds to an MSE of 352.8^2=124,468.

We can also plot the cross-validation scores using the validationplot() function.Using val.type="MSEP", will cause the cv MSE to be plotted.

```{r}
validationplot(pcr.fit,val.type="MSEP")
```

We see that the smallest cv erro occurs when M=16 components are ued. This is barely fewer than M = 19, which amounts to simly performing least squares, because when all of the components are used in PCR no dimension resduction occurs. However, from the plot we also see that the cv erro is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice. 

The summary () function also provides the percentage of variance explained in the predictors and in the responses using different numbers of components. This concept is discussed in greater detail in Chapter 10.

We can think of this as the amount of information about the predictors or setting M=1 only caputures 38.31% of all variance, or information in the predictors. In contract, M=6 increases the value to 88.63%. IF we were to use all M=p=19 components, this would increase to 100%. 

We now perform PCR on the training data and evaluate its test set performance. 

```{r}
set.seed(1)
pcr.fit=pcr(Salary~.,data=Hitters, subset=train, scale=TRUE,
            valudation="CV")
validationplot(pcr.fit, val.type="MSEP")
```

Now, we find that the lowest cv erro occurs when M=7 coponent are used. We compute the test MSE as follows. 
```{r}
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
```

This test set MSE is competitive with the results obtained using ridge ressionand the lasso. However, as a result of they way PCR is impletement, the final model is more difficult to interpret, because it does not perform any kind of variable selection or even direclty produce coefficent estimates.

Finally, we fit PCR on the full data set, using M=7, the number of components identified by cross-validation.

```{r}
pcr.fit=pcr(y~x, scale=TRUE,ncomp=7)
summary(pcr.fit)
```

### 6.7.2 Partial Least Squares 
We impletement partial least squares using the plsr() function, also in the pls library.
The syntax just like tha of the pcr() function.

```{r}
set.seed(1)
require(pls)
require(ISLR)
pls.fit=plsr(Salary~., data=Hitters, subset=train,scale=TRUE,
             validation="CV")
summary(pls.fit)
```

The lowest cross vxalidation error occurs when only M is two partial leastdirectionfs are used.Å@

# Chapter 7: Moving beyond linearity






















































